{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACSU Deep Learning and Vision Reading Group: HW 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome! In this homework we will be implementing a simple PyTorch CNN classifier trained on the CIFAR-10 dataset. There will be a PyTorch tutorial section, where we will implement the classifier and train on CIFAR-10, and also a homework section where we will take a look at how we can improve generalization performance with some regularization techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 0: Brief Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some dependencies out of the way. I assume you have already installed Python 3 (or 2) with numpy. What you will additionally need is `pytorch` along with it's companion `torchvision`. For greatest accessibility, we will be using the CPU-only versions of PyTorch. If you want the GPU acceleration enabled PyTorch bindings, please run the commands here (http://pytorch.org/) according to your CUDA configuration. For CPU-only PyTorch, run the following for Python 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install http://download.pytorch.org/whl/cpu/torch-0.3.1-cp36-cp36m-linux_x86_64.whl \n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the following for Python 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install http://download.pytorch.org/whl/cpu/torch-0.3.1-cp27-cp27mu-linux_x86_64.whl \n",
    "!pip install torchvision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you will be all set. Onward to the tutorial!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Pytorch Tutorial - Learning to Classify CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, let's load pytorch (`import torch`) along with `torchvision` so we can load our dataset. Torchvision is an additional companion package to `torch` that contains useful vision-specific add-ons (for example ResNet50 and Inception-V3 models pretrained on ImageNet). We also want `%matplotlib inline` for future visualizations :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that images from the CIFAR-10 dataset have dimensions `32x32x3` (they are 3 channel RGB images, with 32x32 spatial resolution). There are 50,000 training images and 10,000 test images. Initially, all of the images intensity values are in the range `0..255`. By convention we first normalize this to `0..1` with the following `transforms.ToTensor()` transform. This also converts the images from `PIL` images to torch `Tensor`s, which we will use for fast training (`Tensor`s are abstractions for our data that torch introduces so that it can automatically accelerate computations whenever we use them (e.g. matrix multiplication with tensors)). We further mean normalize each channel by `0.5` and divide by the standard deviation, which is in this case approximated to be `0.5`. The idea is that we normalize our images to be in the range `-1..1`. The transforms can be summarized as doing the following: \n",
    "\n",
    "First `(0..255) / 255 = (0..1)` <br/>\n",
    "then `(0..1) - 0.5 = (-0.5..0.5)` <br/>\n",
    "then `(-0.5..0.5) / 0.5 = (-1..1)`.\n",
    "\n",
    "We do this across all 3 channels of the images, hence the 3-tuples in the `transforms.Normalize` code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`transforms.Compose` will simply apply our transforms in succession to the dataset when we load it in the following way (note that we have specified the above transform with `transform=transform`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code will download the CIFAR-10 dataset to the path specified by `root`. Note that we also have separate concepts of a `dataset` and a `DataLoader`. The dataset is in this case just an abstraction for our data, but when we create a DataLoader we specify a mechanism that will be used to load the data during training. As we cannot afford to load and train on all of it at once, we typically load small batches at a time (`batch_size=4`). It is also considered good practice to shuffle (`shuffle=True`) the data during each epoch (i.e. during each run through all of the data) in order to reduce bias inherent to re-using a particular shuffling of the data.\n",
    "\n",
    "Following are the classes of CIFAR-10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize some images (we can just query our DataLoader for a batch of images, and then we also need to denormalize back to `0..255` for display purposes): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "images, labels = iter(trainloader).next() # get some images\n",
    "images = torchvision.utils.make_grid(images) # sow our batch of images (4x3x32x32) into a grid (3x32x128)\n",
    "images = images / 2 + 0.5 # denormalize\n",
    "images = images.numpy() # go from Tensor to numpy array\n",
    "# switch channel order back from torch Tensor to PIL image: going from 3x32x32 - to 32x32x3\n",
    "images = np.transpose(images, (1, 2, 0)) \n",
    "plt.imshow(images) # display\n",
    "\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4))) # print labels too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. Now that we're a bit more familiar with the data, let's go ahead and define the network we will train to classify our images. First, we'll need some imports. `torch.nn` is the neural network torch module that will let us use a lot of useful layers in our network. `torch.nn.functional` provides some useful functions, like activations (e.g. `ReLU`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define our network. Note that it will inherit from `nn.Module` (as this is the abstraction torch uses for backpropagation), which means we will be required to specify layers/modules are are using in `__init__` and also specify what our network will do when we forward pass the data in `forward`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # note that the three parameters for a conv layer we specify are input_channels,\n",
    "        # output_channels, and kernel_size. In the first conv layer, we are taking 3 input channels (RGB),\n",
    "        # and convlve with a 5x5 kernel to produce a single output channel. The values of the 5x5 kernel\n",
    "        # are the learned parameters in this case, and the 5x5 kernel will become our the 'learned feature'\n",
    "        # after training. We do this for, in this case, 6 different 5x5 kernels to produce 6 output channels.\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        \n",
    "        # max pooling here will just stride a 2x2 window through our entire images and take the highest\n",
    "        # activation value in each window, thereby reducing each dimension by a factor of 2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "        # the linear, or fully connected layer, will take the flattened 16 5x5 output channels of our conv layer,\n",
    "        # and connect them fully to 120 hidden units\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        \n",
    "        # now we just connect the 120 units to another 84 hidden units in the next layer, similar to what we\n",
    "        # do in regular multi-layer perceptrons\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x is the input. A noticeable pattern is that we will convolve, then pass through activations,\n",
    "        # and then pool. This happens dozens of times in larger networks.\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        \n",
    "        # now we flatten the outputs and pass them to the first linear layer. Recall that the purpose\n",
    "        # of linear layers is to give us global context for all of our local conv features.\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go ahead and initialize our network, define a loss function, and define an optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "net = Net() # just initializing an instance of what we built above\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will explain a bit more clearly what specifically the optimizer does in the training code, but know that the `CrossEntropyLoss` is a way of quantifying how closely our network predicts the actual labels of images. If it is `0`, our network predicts the labels perfectly. If it is a very high number, our network is doing a very poor job of classifying the data. \n",
    "\n",
    "We are now ready to get into the training code. I will provide explanationsin the comments as we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# something we'll need for training (explained below)\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# a nice helper function for storing running averages\n",
    "class AverageMeter(object):\n",
    "    #Computes and stores the average and current value\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "# TRAINING STARTS\n",
    "# epoch specifies what pass we are on through the dataset - typically we want to do multiple passes\n",
    "for epoch in range(2):\n",
    "    loss_meter = AverageMeter()\n",
    "    \n",
    "    # note the 0, as we want to start form the 0th batch\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get inputs\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # we now wrap the data Tensors in Variables. Variables are an abstraction around Tensors\n",
    "        # that torch creates so that it can attach relevant gradient information to parameters\n",
    "        # in the network. As our inputs variables are forwarded, all of the other parameters will become\n",
    "        # variables as well because of this initial wrapping. When a tensor t is wrapped as a Variable v,\n",
    "        # we can still access the previous tensor with v.data, and when the gradient is computed with respect\n",
    "        # to that tensor it will be stored in t.grad.data (grad is itself a variable)\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        \n",
    "        # remember how we specified net.parameters() to our optimizer before? This let our optimizer know\n",
    "        # about all trainable parameters in our network. Here it goes through all the trainable, Variable\n",
    "        # wrapped parameters, and zeros their gradients by doing v.grad.data = 0. This is because in pytorch\n",
    "        # gradients are accumulated in the `grad.data`s of your parameters after each iteration, so we want\n",
    "        # to zero them out each time so that we don't accidentally use some old gradients twice\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # first forward pass the batch through our network\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        # now compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # now, what we want to do is lower the loss. To do that, we will need to step in the direction\n",
    "        # of the negative gradient. loss.backward() computes the gradients of our loss with respect to\n",
    "        # each parameter, and actually populates v.grad.data for all trainable parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # the optimizer will then actually update the parameter values using the gradient. It will do roughly\n",
    "        # v.data -= learning_rate * v.grad.data . The learning rate (lr0.001) we defined above for our optimizer\n",
    "        # will control the size of the step we take in our gradient direction, and the momentum\n",
    "        # (momentum=0.9) will control how much of the previous gradient we want to retain in the current step,\n",
    "        # and how much of the new gradient we will add (0.9 means to keep most of the old gradient). The\n",
    "        # optimizer we are using (SGD) is a relatively simple one, and stands for Stochastic Gradient Descent.\n",
    "        # optim.Adam and optim.RMSProp are some more advanced optimizers you might want to try using later.\n",
    "        optimizer.step()\n",
    "        \n",
    "        # now let's just update the loss, and print some stats if needed, and we're ready for the next batch\n",
    "        loss_meter.update(loss.data[0])\n",
    "        \n",
    "        if i % 2000 == 0: # print the loss every 2000 batches\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch+1, i+1, loss_meter.val))\n",
    "            loss_meter.reset()\n",
    "\n",
    "    # uncomment when you're done with the assignment and want to run this code for longer!\n",
    "    # print('Testing epoch', (epoch+1))\n",
    "    # test(net)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, let's quickly sample a few test images and see if our network can correctly predict them. First we load the data like we did previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = iter(testloader).next() # get some images\n",
    "images_copy = images.clone()\n",
    "images = torchvision.utils.make_grid(images) # sow our batch of images (4x3x32x32) into a grid (3x32x128)\n",
    "images = images / 2 + 0.5 # denormalize\n",
    "images = images.numpy() # go from Tensor to numpy array\n",
    "# switch channel order back from torch Tensor to PIL image: going from 3x32x32 - to 32x32x3\n",
    "images = np.transpose(images, (1, 2, 0)) \n",
    "plt.imshow(images) # display\n",
    "\n",
    "print('Correct labels: ', ' '.join('%5s' % classes[labels[j]] for j in range(4))) # print labels too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's forward these through our trained network and see what our predicted labels are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(Variable(images_copy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each image, we will have an output that is in fact a vector of 10 values. Each value corresponds to the likelihood our network assigns to the image being that particular class. For example, let us take a look at the distribution of these likelihoods for the first image by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = images_copy.numpy()[0]\n",
    "img = np.transpose((img / 2 + 0.5), (1, 2, 0))\n",
    "correct_class = labels[0]\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 8))\n",
    "fig.sca(ax1)\n",
    "p = outputs.data[0].numpy()\n",
    "p = np.exp(p - np.max(p))\n",
    "p = p / p.sum(axis=0)\n",
    "ax1.imshow(img)\n",
    "fig.sca(ax1)\n",
    "topk = list(p.argsort()[-10:][::-1])\n",
    "topprobs = p[topk]\n",
    "barlist = ax2.bar(range(10), topprobs)\n",
    "if correct_class in topk:\n",
    "    barlist[topk.index(correct_class)].set_color('g')\n",
    "plt.sca(ax2)\n",
    "plt.ylim([0, 1.1])\n",
    "plt.xticks(range(10),\n",
    "           [classes[i][:15] for i in topk],\n",
    "           rotation='vertical')\n",
    "fig.subplots_adjust(bottom=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The green bar indicates the likelihood for the correct class - notice that it is not necessarily the highest, because the classifier still makes errors. We will take the index with the highest value as the predicted class of our network, and use the `classes` array we previously defined to recover the actual class label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.max returns both the max and the argmax - we only care about the argmax, since that indicates\n",
    "# what indices have the greatest likelihood value\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "# now we look up the highest likelihood indices and print their classes\n",
    "print('Predicted labels: ', ' '.join('%5s' % classes[predicted[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad I guess. Let's compute how we did over the entire test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for i, data in enumerate(testloader, 0):\n",
    "    images, labels = data\n",
    "    outputs = net(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "    \n",
    "print('Accuracy on all 10000 test images: %d %%' % (100 * correct/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 2 epochs of training, that doesn't look too bad. Recall that random guessing on this 10 class dataset would produce an accuracy of 10%, but we are substantially above that. Let's also take a quick look at per class accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "for i, data in enumerate(testloader, 0): \n",
    "    images, labels = data\n",
    "    outputs = net(Variable(images))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    c = (predicted == labels).squeeze()\n",
    "    for i in range(4):\n",
    "        class_correct[labels[i]] += c[i]\n",
    "        class_total[labels[i]] += 1\n",
    "        \n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s: %2d %% ' % (classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some classes are unfortunately substantially below others in terms of accuracy. It is important that you are aware of how your performance is distributed across classes, as most of the time it will not be uniform across everthing. Let's take a look at what else we can add to our basic classifier composed of conv and linear layers. Onward to the regularization homework portion of these notes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Improving our Classifier with Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may recall from lecture, using Dropout and BatchNorm is very common practice when training deep neural networks. Dropout is an explicit regularization method, and although the primary purpose of BatchNorm is to speedup and improve training, it also acts as an implicit regularizer. The issue with our former network is that typically, if we let it run too long, it will begin to overfit to our training data, and fail to generalize to the test data as well as it potentially could (accuracy on test data will start to decrease!). This problem happens because often, deep learning models have so many parameters that they can learn to memorize the specific training data that we give them with more training time, as opposed to learning traits of that data that will help them generalize to test data. We will take a look at how Dropout and BatchNorm help us avoid overfitting and reach higher accuracies with more epochs by limiting the (sometimes problematically large) expressive power of deep neural networks in the following experiments. Since we'll be evaluating a lot in this section on different networks, let's make a function for our previous testing code and let it evaluate an arbitrary network that we specify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(network):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(testloader, 0):\n",
    "        images, labels = data\n",
    "        outputs = network(Variable(images))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "\n",
    "    print('Accuracy on all 10000 test images: %d %%' % (100 * correct/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define a train function that will take in and train with a specified network and evaluate at the end of each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, n_epochs=2):\n",
    "    for epoch in range(n_epochs):\n",
    "        loss_meter = AverageMeter()\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = network(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_meter.update(loss.data[0])\n",
    "            if i % 2000 == 0: # print the loss every 2000 batches\n",
    "                print('[%d, %5d] loss: %.3f' % (epoch+1, i+1, loss_meter.val))\n",
    "                loss_meter.reset()\n",
    "        print('Testing epoch', (epoch+1))\n",
    "        test(network)\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) First, let's go ahead and try out Dropout. Given that in pytorch we use a Dropout layer for convolutions, specified as `nn.Dropout(p=0.5)`, where you set `p` to the probability that any given node is zeroes out during a forward pass, add dropout after the convolutional layers in our network. Similarly, we can add Dropout after linear layers with `nn.Dropout(p=0.5)`, however do not do so below to avoid **over-regularization** (see note on over-regularization at the end):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DropoutNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        # start: add a batchnorm layer here:\n",
    "        self.dr1 = nn.Dropout(p=0.5)\n",
    "        # end\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # start: add a batchnorm layer here:\n",
    "        self.dr2 = nn.Dropout(p=0.5)\n",
    "        # end\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        # DO NOT add a batch norm layer here (in general, ever). We do not mess with the outputs\n",
    "        # as they will be directly compared to the ground truth labels in the loss function.\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # start: change this code to use your Dropout layers\n",
    "        x = self.pool(F.relu(self.dr1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.dr2(self.conv2(x))))\n",
    "        # end\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an important note. Now that you have added Dropout to your network, we need to make a distinction between the train and test modes of your network. During training, the Dropout layers will automatically zero out any given node with specified probability `p`. During testing, we do not want this to happen, as we just want to evaluate and use all information from the network. Conveniently, PyTorch provides us with `net.train()` and `net.eval()` (for testing) so that we can automatically turn on or off whether or not we want nodes to be stochastically zeroed out. We will see in a bit that we also need `net.train()` and `net.eval()` for BatchNorm. For the time being, let's go ahead and train your DropoutNet for 2 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DropoutNet()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) # set our optimizer to optimize our new net\n",
    "net.train() # we enable dropout during training\n",
    "train(net, 2) # train the network for 2 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great, as a result of being limited by CPU resources (only running for 2 epochs) :(. If you have the time, run both the original network and the DropoutNet for a larger number of epochs, say 50, and note that although the DropoutNet takes longer to train/reach the same accuracy, it does in fact eventually reach a higher accuracy than the original and does not suffer from overfitting in the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Now, let's go ahead and try out BatchNorm. Given that in PyTorch we use a BatchNorm2d layer for convolutions, specified as `nn.BatchNorm2d(n)`, where `n` is the number of output channels in the convolution, add BatchNorm for convolution layers to the previous network. Similarly, for linear layers, we would use `nn.BatchNorm1d(n)`, where n is the number of output units of the linear layer. However, please only add BatchNorm for the convolutional layers, because we do not want to **over-regularize** (see note at the end about over-regularization). Please add the BatchNorm **after** the ReLU activation (even though the original BatchNorm paper suggests putting it before, empirically it has been shown to work better when placed after the activation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BatchNormNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        # start: add a batchnorm layer here:\n",
    "        self.bn1 = nn.BatchNorm2d(6)\n",
    "        # end\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # start: add a batchnorm layer here:\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        # end\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        # DO NOT add a batch norm layer here. We do not mess with the outputs\n",
    "        # as they will be directly compared to the ground truth labels in the loss function.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # start: change this code to use your conv Batchnorm layers\n",
    "        x = self.pool(self.bn1(F.relu(self.conv1(x))))\n",
    "        x = self.pool(self.bn2(F.relu(self.conv2(x))))\n",
    "        # end\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important note. Now that you have added BatchNorm to your network, we need to make a distinction between the train and test modes of your network. During training, the BatchNorm layers will automatically update their parameters (mean and variance) upon each forward pass. During testing, we do not want this to happen, as we just want to evaluate. Conveniently, the PyTorch functions `net.train()` and `net.eval()` (for testing) also automatically turn on or off whether or not BatchNorm parameters are being updated upon each forward pass. For the time being, let's go ahead and train your BatchNormNet for 2 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BatchNormNet()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) # set our optimizer to optimize our new net\n",
    "net.train() # we let the BatchNorm parameters update during forward passes\n",
    "train(net, 2) # train the network for 2 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad - seems to even be better than the original network for only 2 epochs. If you have the chance to, run it for longer and see how good the results you can get are! Also feel free to tweak the hyperparameters for the convolutional and fully-connected layers. I want to also make a slight note about potential issues. \n",
    "\n",
    "**Over-regularlization**. Over-regularizing is possible, and happens when you regularize your network so much that it is unable to learn anything. It may help to decrease the number of dropout layers or batchnorm layers you use if you see that your network that was previously able to decrease in training loss now fails to do so (after adding regularization). Usually you detect over-regularization empirically when your training loss fails to decrease, whereas previously (before regularization) is was able to decrease with training time.\n",
    "\n",
    "Hopefully we were able to demonstrate that even on our toy CIFAR-10 example, regularization in the form of Dropout helps prevent overfitting and that Batchnorm helps make training faster (and also acts as an implicit regularizer). Quick side note: on larger networks, most people nowadays just use BatchNorm (not Dropout) as it is usually sufficient to just add Batchnorm (instead of Batchnorm AND Dropout) for regularization purposes. On smaller networks Dropout can still make a significant difference however, so I encourage you to use it in practice if it gives you good empirical results. Anyways, I think we're done for now. I'll see you next class! -Isay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**. Parts of the tutorial section were modified from Adam Paszke's tutorial here: https://github.com/pytorch/tutorials/blob/master/beginner_source/blitz/cifar10_tutorial.py and also Anish Athalye's blog post here: https://www.anishathalye.com/2017/07/25/synthesizing-adversarial-examples/ ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
